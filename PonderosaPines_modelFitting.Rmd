---
title: "Ponderosa Pines"
author: "Sarah Hamilton"
date: "2023-11-14"
output: html_document
---

```{r}
rm(list = ls()) # this clears out everything like our clear command in matlab
```

# Model fitting - Ponderosa Pines
In the following data,  is the diameter of a ponderosa pine in inches measured at chest height and  is a measure of volume (number of board feet divided by 10).  
```{r}
x   = c(17,19,20,22,23,25,28,31,32,33,36,37,39,42)
y   = c(19,25,32,51,57,71,113,140,153,187,192,205,250,260)
```
Let's test out the following models 
a) $f1(x)=ax+b$
b) $f2(x)=ax^2$
c) $f3(x)=ax^2 + bx + c$
d) $f4(x) = ax^3 + bx^2 +c$ - careful, this one is NOT just a generic cubic.

Ok, let's get to work!

## Part 0: Plot the data:
Let's start by just visualizing the data:
```{r}
plot(x,y,col="blue",pch=16, main="Data for the ponderosa pine",xlab="diameter",ylab="volume")
```

## Part (a):  Fitting $f1(x)=ax+b$
Let's use R's built in linear model creator **lm()** to compute this (remember we can also use the formulas derived in class)
```{r}
df = data.frame(x,y)
f1 = lm(formula = y~x,data =df)
summary(f1)df = data.frame(x,y)

```

We can look at the fit at the data points visually and in a table:
```{r}
f1_eval = predict(f1,data.frame(x))
plot(x,y,col="blue",pch=16, main="Does a linear model (ax+b) fit the data?",xlab="diameter",ylab="volume")
points(x, f1_eval, type="o",col="red", pch="o")
legend("topleft", legend=c("data", "linear model"), col=c("blue", "red"), pch=c(16,1),lty=0:1, cex=0.8)
```

Let's look at a table of the errors and also plot them:
```{r}
Fit        = f1_eval
FitError   = y-f1_eval
data.frame(x,y,Fit,FitError)
```

Notice also that R already computed the residuals for us
```{r}
data.frame(x,y,Fit,f1$residuals)
```

Also we can see very quickly that the $R^2$ value is excellent for this model at about 0.98.

Let's plot these residual errors:
```{r}
plot(x,f1$residuals,col="red",pch=16, main="Residuals from the linear model",xlab="diameter",ylab="residual: y-f(x)")
```

Looking at the errors there's not an obvious pattern but it also doesn't just look like random noise.... Many of the data points are being overestimated (producing usually more negative residuals than positive).  Let's see if we can do better.

## Part b) Power function: $f2(x)=ax^2$
Recall in class we derived this formula: $a = \frac{\sum x_i^n y_i}{\sum x_i^{2n}}$ and we can use it to compute the best value for $a$ for this model with 
```{r}
n = 2;
a = sum((x^n)*y)/sum(x^(2*n))
paste("Our model gives f2(x) = ax^2 = ", a,"x^2.")
```

Again, plotting and looking at the errors:
```{r}
f2      = a*x^2;
x_fine  = seq(from = min(x), to = max(x), by = 1)
f2_fine = a*x_fine^2 # evaluate f2 over a finer grid to see the fit overall

plot(x,y,col="blue",pch=16, main="Does a power model (ax^2) fit the data?",xlab="diameter",ylab="volume") 

points(x, f2, col="red", lty=2)
lines(x_fine,f2_fine, col="red",pch="o")

legend("topleft", legend=c("data", "power model"), col=c("blue", "red"), pch=c(16,1),lty=0:1, cex=0.8)

```

Visually we can immediately see this fit is not good, but we can still look at the residual plots and a table of them:
Let's look at a table of the errors and also plot them:
```{r}
Fit        = f2
FitError   = y-f2
data.frame(x,y,Fit,FitError)
```

Let's plot these residual errors:
```{r}
plot(x,FitError,col="red",pch=16, main="Residuals from the power model (ax^2)",xlab="diameter",ylab="residual: y-f(x)")
```

here the errors definitely seem to have a structure (increasing, looking almost linear).  This would suggest to me that we can do better than this model and we're missing something important.  


## Part c): $f_3(x)=ax^2 + bx + c$
We can use R's built in poly function to handle this.  Note that I set **raw=T** to use traditional polynomials (vs orthogonal(?) ones.  This way it matches what we expect by hand, from polyfit in Matlab, and also the Vandermonde approach.)
```{r}
f3 = lm(formula = y~poly(x,2,raw=T),data=df) 
summary(f3)
```

We can look at the fit at the data points visually and in a table:
```{r}
f3_eval = predict(f3,data.frame(x))
plot(x,y,col="blue",pch=16, main="Does a quadratic model (ax^2 + bx + c) fit the data?",xlab="diameter",ylab="volume")
points(x, f3_eval, type="o",col="red", pch="o")
legend("topleft", legend=c("data", "quadratic model"), col=c("blue", "red"), pch=c(16,1),lty=0:1, cex=0.8)
```

Again we can use the residuals that **lm()** already created for us:

```{r}
Fit        = f3_eval
FitError   = f3$residuals
data.frame(x,y,Fit,FitError)
```
and plot them as well:

Let's plot these residual errors:
```{r}
plot(x,f3$residuals,col="red",pch=16, main="Residuals from the quadratic model",xlab="diameter",ylab="residual: y-f(x)")
```
## Part d) Model: $f4=ax^3 + bx^2 + c$
You can work out, by hand, the formulas for the system of linear equations:
```{r}
b = matrix(c(sum((x^3)*y), sum((x^2)*y), sum(y)), nrow=3, ncol=1)

A = matrix(1:9, nrow=3, ncol=3)
A[1,1:3] = c(sum(x^6),sum(x^5),sum(x^4))
A[2,1:3] = c(sum(x^5), sum(x^4), sum(x^2))
A[3,1:3] = c(sum(x^3), sum(x^2), length(x))
```
We can now solve the system  using R's **solve** command
```{r}
coeffs = solve(A,b)
print(coeffs)
```
These also match what we found with Matlab.  We can then predict the value of the model at the data points and plot overall:
```{r}
f4      = coeffs[1]*x^3 + coeffs[2]*x^2 + coeffs[3] # evaluate the model at the data points
f4_fine = coeffs[1]*x_fine^3 + coeffs[2]*x_fine^2 + coeffs[3] # evaluate the model at a set of finer points for plotting.

plot(x,y,col="blue",pch=16, main="Does a power model (ax^2) fit the data?",xlab="diameter",ylab="volume") 

points(x, f4, col="red", lty=2)
lines(x_fine,f4_fine, col="red",pch="o")

legend("topleft", legend=c("data", "ax^3 + bx^2 +c  model"), col=c("blue", "red"), pch=c(16,1),lty=0:1, cex=0.8)

```

Visually we can immediately see this fit is not good, but we can still look at the residual plots and a table of them:
Let's look at a table of the errors and also plot them:
```{r}
Fit        = f4
FitError   = y-f4
data.frame(x,y,Fit,FitError)
```
Let's plot these residual errors:
```{r}
plot(x,FitError,col="red",pch=16, main="Residuals from the power model (ax^3 + bx^2 + c)",xlab="diameter",ylab="residual: y-f(x)")
```

Remember to look at the scale.  This model has pretty large errors, especially for diameters over 40 and around 33.
git hub is awesome

